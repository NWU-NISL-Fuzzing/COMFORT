import os
import time
import datetime
import shutil
import math
from uuid import uuid4
from generate_model.callable_processor import CallableProcessor
from utils.config import Hparams_Generator
import gpt_2_simple as gpt2


def info_print(str):
    print(f"\033[1;34;48m{str}\033[0m")


# init params
hparams = Hparams_Generator().parser.parse_args()
if hparams.multi_gpu == 1:
    info_print('GPU enabled.')
    hparams.multi_gpu = True  # enable gpu
else:
    info_print('GPU disabled.')
    hparams.multi_gpu = False
    os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # disable gpu
time.sleep(1)

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Mask prompts for TensorFlow


def gpt2_finetune(hparams):
    info_print("Model finetuning, please wait. (Press Ctrl+C to exit early)")
    sess = gpt2.start_tf_sess()

    # input check
    if not os.path.exists(os.path.join(hparams.gpt2_model_dir, hparams.gpt2_model_name)):
        raise FileNotFoundError("The specified gpt2 pretrained model doesn't exist, please restore the default params.")

    # clear checkpoint dir
    model_path = os.path.join(hparams.finetuned_model_dir, hparams.finetuned_model_name)
    if os.path.exists(model_path):
        shutil.rmtree(model_path)

    gpt2.finetune(sess=sess,
                  dataset=hparams.data_path,
                  model_dir=hparams.gpt2_model_dir,
                  model_name=hparams.gpt2_model_name,
                  checkpoint_dir=hparams.finetuned_model_dir,
                  run_name=hparams.finetuned_model_name,
                  multi_gpu=hparams.multi_gpu,
                  steps=hparams.steps)


def function_generate(hparams, saved_dir):
    sess = gpt2.start_tf_sess()
    sess = gpt2.reset_session(sess)

    # determine the generate model to use
    if hparams.use_nisl_model == 1:
        info_print('The NISL model is selected.')
        generate_model_dir = hparams.gpt2_model_dir
        generate_model_name = 'nisl_model'
    else:
        # model check
        info_print('The new model is selected.')
        generate_model_dir = hparams.finetuned_model_dir
        generate_model_name = hparams.finetuned_model_name
        if not os.path.exists(os.path.join(os.path.join(generate_model_dir, generate_model_name), 'checkpoint')):
            raise FileNotFoundError("The specified model doesn't exist, please finetune first or set 'use_nisl_model=1'")
    time.sleep(1)

    info_print("Generating JS test program (approx 15 minutes with gpus when nsamples=512 - including model load time)...\n")
    gpt2.load_gpt2(sess,
                   model_dir=generate_model_dir,
                   model_name=generate_model_name,
                   multi_gpu=hparams.multi_gpu)

    assert hparams.batch_size != 0, "'batch_size' cannot be 0!"
    batches = int(math.ceil(hparams.nsamples / hparams.batch_size))
    remainder = hparams.nsamples % hparams.batch_size

    all_functions = []

    for idx in range(batches):
        try:
            texts = gpt2.generate(sess,
                                 model_dir=generate_model_dir,
                                 model_name=generate_model_name,
                                 nsamples=hparams.batch_size,
                                 batch_size=hparams.batch_size,
                                 prefix=hparams.generate_prefix,
                                 top_p=hparams.top_p,
                                 top_k=hparams.top_k,
                                 temperature=hparams.temperature,
                                 include_prefix=True,
                                 return_as_list=True)

            # when the last batch, intercepted by the remainder
            if idx == batches - 1:
                if remainder != 0:
                    texts = texts[:remainder]

            for text in texts:
                functions = text.split(hparams.generate_prefix)[1:]

                # get rid of the last one to prevent syntax errors
                if len(functions) >= 2:
                    functions = functions[:-1]

                all_functions += functions

                # print
                for function in functions:
                    print(function)
                    print('=' * 50)

            info_print(f'Generating {idx+1}/{batches}.')

        except:
            continue

    # formatting
    all_functions = [i.strip() + '\n' for i in all_functions]

    # save all generated functions by gpt2 to a new file
    if not os.path.exists(saved_dir):
        os.makedirs(saved_dir)
    for idx, function in enumerate(all_functions, start=1):
        with open(os.path.join(saved_dir, f'{idx}.js'), 'w', encoding='utf-8') as f:
            f.write(function)

    return all_functions


def testcase_assemble(functions, saved_dir):
    info_print(f"Assembly test program (approx 30 seconds)...\n")
    time.sleep(2)
    callable_processor = CallableProcessor()
    testcases = []
    for function in functions:
        try:
            testcases.append(callable_processor.get_self_calling(function))
        except:
            testcases.append('')  # Guarantee idx consistent

    # print info
    info_print(f"Test program after creating the calling routine: \n")
    for testcase in testcases:
        print(testcase)
        print('=' * 50)

    # formatting
    testcases = [i.strip() + '\n' for i in testcases]

    # save all testcase to a new file
    if not os.path.exists(saved_dir):
        os.makedirs(saved_dir)
    for idx, testcase in enumerate(testcases, start=1):
        with open(os.path.join(saved_dir, f'{idx}.js'), 'w', encoding='utf-8') as f:
            f.write(testcase)

    return testcases


if __name__ == '__main__':
    if hparams.mode == 'finetune':
        gpt2_finetune(hparams)
    elif hparams.mode == 'generate':
        start_time = time.time()

        # new a directory name
        time_stamp = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')
        identifier = str(uuid4())[:8]
        new_dir_name = f'{time_stamp}_{identifier}'

        # generate JS function by gpt2
        samples_saved_dir = os.path.join(hparams.sample_dir, new_dir_name)
        generated_functions = function_generate(hparams, samples_saved_dir)

        # assemble JS testcase
        testcases_saved_dir = os.path.join(hparams.testcase_dir, new_dir_name)
        testcase_assemble(generated_functions, testcases_saved_dir)
        end_time = time.time()
        info_print(f"A total of {len(generated_functions)} testcases were generated, taking {int(end_time-start_time)} seconds.")
        info_print(f"All generated functions by gpt2 are saved in: {samples_saved_dir}")
        info_print(f"All generated testcases are saved in: {testcases_saved_dir}")

    else:
        raise ValueError('The "mode" parameter is invalid. Please enter it again!')
